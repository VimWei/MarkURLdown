from __future__ import annotations

from dataclasses import dataclass
import time
import random

from bs4 import BeautifulSoup

from markitdown_app.core.html_to_md import html_fragment_to_markdown
from markitdown_app.core.common_utils import get_user_agents, extract_title_from_html
# æ³¨æ„ï¼šcrawlersæ¨¡å—å·²åˆ é™¤ï¼Œä½¿ç”¨å†…è”å®ç°


@dataclass
class CrawlerResult:
    """çˆ¬è™«ç»“æœ"""
    success: bool
    title: str | None
    text_content: str
    error: str | None = None


@dataclass
class FetchResult:
    title: str | None
    html_markdown: str


def fetch_zhihu_article(session, url: str) -> FetchResult:
    """
    è·å–çŸ¥ä¹ä¸“æ æ–‡ç« å†…å®¹ - å¤šç­–ç•¥å°è¯•
    
    ä½¿ç”¨å¤šç§çˆ¬è™«æŠ€æœ¯ï¼ŒæŒ‰ä¼˜å…ˆçº§å°è¯•ï¼š
    1. Playwright - ç°ä»£åŒ–æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆæœ€å¯é ï¼Œèƒ½å¤„ç†çŸ¥ä¹éªŒè¯ï¼‰
    2. httpx - ç°ä»£åŒ–HTTPå®¢æˆ·ç«¯ï¼ˆå¤‡ç”¨ç­–ç•¥ï¼‰
    """
    
    # å®šä¹‰çˆ¬è™«ç­–ç•¥ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    # ä¼˜å…ˆä½¿ç”¨Playwrightå¤„ç†éœ€è¦éªŒè¯çš„é“¾æ¥ï¼Œç„¶åä½¿ç”¨è½»é‡çº§ç­–ç•¥
    crawler_strategies = [
        # ç­–ç•¥1: Playwright - æœ€å¯é ï¼Œèƒ½å¤„ç†çŸ¥ä¹çš„éªŒè¯æœºåˆ¶
        lambda: _try_playwright_crawler(url),
        
        # ç­–ç•¥2: httpx - ç°ä»£åŒ–HTTPå®¢æˆ·ç«¯ï¼Œé€Ÿåº¦å¿«ï¼ˆå¤‡ç”¨ç­–ç•¥ï¼‰
        lambda: _try_httpx_crawler(session, url),
        
    ]
    
    # å°è¯•å„ç§ç­–ç•¥ï¼Œå¢åŠ é‡è¯•æœºåˆ¶
    max_retries = 2  # æ¯ä¸ªç­–ç•¥æœ€å¤šé‡è¯•2æ¬¡
    
    for i, strategy in enumerate(crawler_strategies, 1):
        for retry in range(max_retries):
            try:
                if retry > 0:
                    print(f"å°è¯•çŸ¥ä¹è·å–ç­–ç•¥ {i} (é‡è¯• {retry}/{max_retries-1})...")
                    time.sleep(random.uniform(3, 6))  # é‡è¯•æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                else:
                    print(f"å°è¯•çŸ¥ä¹è·å–ç­–ç•¥ {i}...")
                
                result = strategy()
                if result.success:
                    # å¤„ç†å†…å®¹å¹¶æ£€æŸ¥è´¨é‡
                    processed_result = _process_zhihu_content(result.text_content, result.title, url)
                    
                    # æ£€æŸ¥æ˜¯å¦è·å–åˆ°éªŒè¯é¡µé¢ - æ›´ç²¾ç¡®çš„æ£€æµ‹
                    content = processed_result.html_markdown or ""
                    if content and len(content) > 1000:  # å¦‚æœå†…å®¹è¶³å¤Ÿé•¿ï¼Œè¯´æ˜ä¸æ˜¯éªŒè¯é¡µé¢
                        print(f"ç­–ç•¥ {i} æˆåŠŸè·å–åˆ°å†…å®¹!")
                        return processed_result
                    elif content and ("éªŒè¯" in content or "ç™»å½•" in content or "è®¿é—®è¢«æ‹’ç»" in content or "403" in content or "404" in content):
                        print(f"ç­–ç•¥ {i} è·å–åˆ°éªŒè¯é¡µé¢ï¼Œé‡è¯•...")
                        if retry < max_retries - 1:
                            continue
                        else:
                            print(f"ç­–ç•¥ {i} é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥")
                            break
                    
                    # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«éªŒè¯ä¿¡æ¯
                    if processed_result.title and ("éªŒè¯" in processed_result.title or "ç™»å½•" in processed_result.title or "è®¿é—®è¢«æ‹’ç»" in processed_result.title):
                        print(f"ç­–ç•¥ {i} æ ‡é¢˜åŒ…å«éªŒè¯ä¿¡æ¯ï¼Œé‡è¯•...")
                        if retry < max_retries - 1:
                            continue
                        else:
                            print(f"ç­–ç•¥ {i} é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œå°è¯•ä¸‹ä¸€ä¸ªç­–ç•¥")
                            break
                    
                    print(f"ç­–ç•¥ {i} æˆåŠŸ!")
                    return processed_result
                else:
                    print(f"ç­–ç•¥ {i} å¤±è´¥: {result.error}")
                    if retry < max_retries - 1:
                        continue
                    else:
                        break
            except Exception as e:
                print(f"ç­–ç•¥ {i} å¼‚å¸¸: {e}")
                if retry < max_retries - 1:
                    continue
                else:
                    break
        
        # ç­–ç•¥é—´ç­‰å¾…
        if i < len(crawler_strategies):
            time.sleep(random.uniform(2, 4))
    
    # æ‰€æœ‰ç­–ç•¥éƒ½å¤±è´¥ï¼Œæä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œç”¨æˆ·æŒ‡å¯¼
    print("âš ï¸  çŸ¥ä¹æ–‡ç« çˆ¬å–é‡åˆ°é™åˆ¶")
    print("ğŸ’¡ å»ºè®®:")
    print("   1. å°è¯•ä½¿ç”¨VPNæˆ–ä»£ç†")
    print("   2. è”ç³»æ–‡ç« ä½œè€…è·å–æˆæƒ")
    print("   3. ä½¿ç”¨å…¶ä»–å·¥å…·æ‰‹åŠ¨å¤åˆ¶å†…å®¹")
    print("   4. å°è¯•åœ¨æµè§ˆå™¨ä¸­ç›´æ¥è®¿é—®æ–‡ç« ")
    raise Exception("çŸ¥ä¹æ–‡ç« çˆ¬å–å¤±è´¥ï¼Œè¯·å°è¯•å…¶ä»–æ–¹æ³•")


def _try_playwright_crawler(url: str) -> CrawlerResult:
    """å°è¯•ä½¿ç”¨ Playwright çˆ¬è™« - èƒ½å¤„ç†çŸ¥ä¹çš„éªŒè¯æœºåˆ¶"""
    try:
        # åŠ¨æ€å¯¼å…¥ Playwright
        from playwright.sync_api import sync_playwright
        
        with sync_playwright() as p:
            # å¯åŠ¨çœŸå®çš„Chromeæµè§ˆå™¨ï¼Œä½¿ç”¨æˆåŠŸçš„åæ£€æµ‹é…ç½®
            browser = p.chromium.launch(
                headless=False,  # ä½¿ç”¨éheadlessæ¨¡å¼ä»¥ç»•è¿‡æ£€æµ‹
                channel="chrome",  # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„Chrome
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-gpu',
                    '--no-first-run',
                    '--no-default-browser-check',
                    '--disable-extensions',
                    '--disable-plugins',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding',
                ]
            )
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·ç¯å¢ƒ
            context = browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='zh-CN',
                timezone_id='Asia/Shanghai',
                geolocation={'latitude': 39.9042, 'longitude': 116.4074},  # åŒ—äº¬åæ ‡
                permissions=['geolocation'],
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1',
                    'Cache-Control': 'max-age=0',
                }
            )
            
            # åˆ›å»ºé¡µé¢
            page = context.new_page()
            
            # æ³¨å…¥åæ£€æµ‹è„šæœ¬
            page.add_init_script("""
                // éšè—webdriverå±æ€§
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
                
                // æ¨¡æ‹ŸçœŸå®çš„navigatorå±æ€§
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5],
                });
                
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['zh-CN', 'zh', 'en'],
                });
                
                // æ¨¡æ‹ŸçœŸå®çš„å±å¹•å±æ€§
                Object.defineProperty(screen, 'width', {
                    get: () => 1920,
                });
                
                Object.defineProperty(screen, 'height', {
                    get: () => 1080,
                });
                
                // æ¨¡æ‹ŸçœŸå®çš„æ—¶åŒº
                Object.defineProperty(Intl.DateTimeFormat.prototype, 'resolvedOptions', {
                    value: function() {
                        return { timeZone: 'Asia/Shanghai' };
                    }
                });
            """)
            
            # è®¾ç½®è¶…æ—¶
            page.set_default_timeout(30000)
            
            # è®¿é—®é¡µé¢
            print(f"Playwright: æ­£åœ¨è®¿é—® {url}")
            
            # ä¼˜åŒ–çš„è®¿é—®æµç¨‹ï¼šé¦–é¡µ â†’ ç›®æ ‡æ–‡ç« 
            try:
                # 1. å…ˆè®¿é—®çŸ¥ä¹é¦–é¡µå»ºç«‹ä¼šè¯
                print("Playwright: æ­£åœ¨è®¿é—®çŸ¥ä¹é¦–é¡µå»ºç«‹ä¼šè¯...")
                home_response = page.goto("https://www.zhihu.com/", wait_until='domcontentloaded', timeout=15000)
                if home_response and home_response.status == 200:
                    print("Playwright: çŸ¥ä¹é¦–é¡µè®¿é—®æˆåŠŸï¼Œè·å–cookies...")
                    # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·è¡Œä¸ºï¼šç­‰å¾…é¡µé¢åŠ è½½
                    page.wait_for_timeout(random.uniform(2000, 4000))
                    
                    # æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨
                    page.mouse.move(random.randint(100, 800), random.randint(100, 400))
                    page.wait_for_timeout(random.uniform(500, 1500))
                    
                    # æ™ºèƒ½å¤„ç†é¦–é¡µç™»å½•å¼¹çª—
                    try:
                        # ç­‰å¾…å¼¹çª—å¯èƒ½å‡ºç°çš„æ—¶æœº
                        page.wait_for_timeout(random.uniform(500, 1500))
                        
                        login_selectors = [
                            '.Modal-closeButton',
                            '.SignFlow-close', 
                            '[aria-label="å…³é—­"]',
                            '.Modal-close',
                            '.close-button',
                            'button[aria-label="å…³é—­"]'
                        ]
                        
                        login_close = None
                        for selector in login_selectors:
                            login_close = page.query_selector(selector)
                            if login_close:
                                break
                        
                        if login_close:
                            print("Playwright: å‘ç°é¦–é¡µç™»å½•å¼¹çª—ï¼Œå°è¯•å…³é—­...")
                            
                            # å¤šç§å…³é—­ç­–ç•¥
                            try:
                                login_close.click(timeout=3000)
                                print("Playwright: é¦–é¡µå¼¹çª—ç›´æ¥ç‚¹å‡»æˆåŠŸ")
                            except:
                                try:
                                    login_close.click(force=True, timeout=2000)
                                    print("Playwright: é¦–é¡µå¼¹çª—å¼ºåˆ¶ç‚¹å‡»æˆåŠŸ")
                                except:
                                    try:
                                        page.evaluate("arguments[0].click()", login_close)
                                        print("Playwright: é¦–é¡µå¼¹çª—JavaScriptç‚¹å‡»æˆåŠŸ")
                                    except:
                                        page.keyboard.press('Escape')
                                        print("Playwright: é¦–é¡µå¼¹çª—ä½¿ç”¨ESCé”®å…³é—­")
                            
                            page.wait_for_timeout(500)
                        else:
                            print("Playwright: é¦–é¡µæœªå‘ç°ç™»å½•å¼¹çª—")
                    except Exception as e:
                        print(f"Playwright: å¤„ç†é¦–é¡µç™»å½•å¼¹çª—å¼‚å¸¸: {e}")
                else:
                    print("Playwright: çŸ¥ä¹é¦–é¡µè®¿é—®å¤±è´¥")
            except Exception as e:
                print(f"Playwright: è®¿é—®çŸ¥ä¹é¦–é¡µå¤±è´¥: {e}")
            
            # 2. ç›´æ¥è®¿é—®ç›®æ ‡æ–‡ç« 
            print("Playwright: ç›´æ¥è®¿é—®ç›®æ ‡æ–‡ç« ...")
            response = page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # æ™ºèƒ½ç­‰å¾…é¡µé¢ç¨³å®šå¹¶å¤„ç†ç™»å½•å¼¹çª—
            print("Playwright: ç­‰å¾…é¡µé¢ç¨³å®šå¹¶å¤„ç†ç™»å½•å¼¹çª—...")
            
            # ç­‰å¾…é¡µé¢åŸºæœ¬åŠ è½½å®Œæˆ
            page.wait_for_timeout(random.uniform(1000, 2000))
            
            # æ™ºèƒ½å¤„ç†ç™»å½•å¼¹çª— - å¤šç§ç­–ç•¥
            try:
                # æŸ¥æ‰¾å„ç§å¯èƒ½çš„ç™»å½•å¼¹çª—å…³é—­æŒ‰é’®
                login_selectors = [
                    '.Modal-closeButton',
                    '.SignFlow-close', 
                    '[aria-label="å…³é—­"]',
                    '.Modal-close',
                    '.close-button',
                    'button[aria-label="å…³é—­"]',
                    '.ant-modal-close',
                    '.el-dialog__close'
                ]
                
                login_close = None
                for selector in login_selectors:
                    login_close = page.query_selector(selector)
                    if login_close:
                        break
                
                if login_close:
                    print("Playwright: å‘ç°ç™»å½•å¼¹çª—ï¼Œå°è¯•å…³é—­...")
                    
                    # ç­–ç•¥1: ç›´æ¥ç‚¹å‡»
                    try:
                        login_close.click(timeout=5000)
                        print("Playwright: ç›´æ¥ç‚¹å‡»æˆåŠŸ")
                    except:
                        # ç­–ç•¥2: å¼ºåˆ¶ç‚¹å‡»
                        try:
                            login_close.click(force=True, timeout=3000)
                            print("Playwright: å¼ºåˆ¶ç‚¹å‡»æˆåŠŸ")
                        except:
                            # ç­–ç•¥3: ä½¿ç”¨JavaScriptç‚¹å‡»
                            try:
                                page.evaluate("arguments[0].click()", login_close)
                                print("Playwright: JavaScriptç‚¹å‡»æˆåŠŸ")
                            except:
                                # ç­–ç•¥4: æŒ‰ESCé”®å…³é—­å¼¹çª—
                                try:
                                    page.keyboard.press('Escape')
                                    print("Playwright: ä½¿ç”¨ESCé”®å…³é—­å¼¹çª—")
                                except:
                                    print("Playwright: æ‰€æœ‰å…³é—­ç­–ç•¥éƒ½å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ...")
                    
                    # çŸ­æš‚ç­‰å¾…ç¡®è®¤å¼¹çª—å…³é—­
                    page.wait_for_timeout(1000)
                else:
                    print("Playwright: æœªå‘ç°ç™»å½•å¼¹çª—")
                    
            except Exception as e:
                print(f"Playwright: å¤„ç†ç™»å½•å¼¹çª—å¼‚å¸¸: {e}")
                # å°è¯•æŒ‰ESCé”®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
                try:
                    page.keyboard.press('Escape')
                    print("Playwright: ä½¿ç”¨ESCé”®ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
                except:
                    pass
            
            # æœ€ç»ˆç­‰å¾…é¡µé¢å®Œå…¨ç¨³å®š
            page.wait_for_timeout(random.uniform(1000, 2000))
            
            # æ£€æŸ¥é¡µé¢æ ‡é¢˜è€Œä¸æ˜¯å“åº”çŠ¶æ€ï¼Œå› ä¸ºçŸ¥ä¹å¯èƒ½è¿”å›200ä½†å†…å®¹æ˜¯403é¡µé¢
            try:
                page_title = page.title()
                print(f"Playwright: è·å–åˆ°æ ‡é¢˜: {page_title}")
                if "403" in page_title or "Forbidden" in page_title:
                    browser.close()
                    return CrawlerResult(
                        success=False,
                        title=None,
                        text_content="",
                        error=f"Page title indicates 403: {page_title}"
                    )
            except Exception as e:
                print(f"Playwright: è·å–æ ‡é¢˜å¤±è´¥: {e}")
                # ç»§ç»­æ‰§è¡Œï¼Œå¯èƒ½é¡µé¢æ­£åœ¨åŠ è½½
            
            # æ¨¡æ‹ŸçœŸå®ç”¨æˆ·é˜…è¯»è¡Œä¸º
            page.wait_for_timeout(random.uniform(3000, 6000))
            
            # æ¨¡æ‹Ÿé¼ æ ‡ç§»åŠ¨å’Œæ»šåŠ¨
            page.mouse.move(random.randint(300, 700), random.randint(300, 600))
            page.wait_for_timeout(random.uniform(500, 1500))
            page.mouse.wheel(0, random.randint(100, 400))
            page.wait_for_timeout(random.uniform(1000, 2000))
            
            # å°è¯•ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
            try:
                # ç­‰å¾…å†…å®¹åŒºåŸŸåŠ è½½
                page.wait_for_selector('div.Post-RichTextContainer, div.Post-RichText, article, div.content', timeout=10000)
                print("Playwright: æ‰¾åˆ°å†…å®¹åŒºåŸŸ")
            except:
                print("Playwright: æœªæ‰¾åˆ°å†…å®¹åŒºåŸŸï¼Œä½†ç»§ç»­è·å–é¡µé¢å†…å®¹...")
            
            # è·å–é¡µé¢å†…å®¹
            html = page.content()
            
            # å°è¯•è·å–æ ‡é¢˜
            title = None
            try:
                title = page.title()
                print(f"Playwright: è·å–åˆ°æ ‡é¢˜: {title}")
            except:
                pass
            
            browser.close()
            
            return CrawlerResult(
                success=True,
                title=title,
                text_content=html
            )
            
    except ImportError:
        return CrawlerResult(
            success=False,
            title=None,
            text_content="",
            error="Playwright not installed. Install with: pip install playwright && playwright install"
        )
    except Exception as e:
        return CrawlerResult(
            success=False,
            title=None,
            text_content="",
            error=f"Playwright error: {str(e)}"
        )


def _try_httpx_crawler(session, url: str) -> CrawlerResult:
    """å°è¯•ä½¿ç”¨ httpx çˆ¬è™«"""
    # ä¸ºçŸ¥ä¹è®¾ç½®ç‰¹æ®Šçš„è¯·æ±‚å¤´
    zhihu_headers = _get_zhihu_headers()
    
    try:
        import httpx
        # åˆ›å»ºæ–°çš„httpxä¼šè¯ï¼Œä½¿ç”¨çŸ¥ä¹ä¸“ç”¨è¯·æ±‚å¤´
        with httpx.Client(
            timeout=30,
            follow_redirects=True,
            headers=zhihu_headers
        ) as client:
            # å…ˆè®¿é—®çŸ¥ä¹é¦–é¡µå»ºç«‹ä¼šè¯å’Œè·å–cookies
            try:
                print("httpx: æ­£åœ¨è®¿é—®çŸ¥ä¹é¦–é¡µå»ºç«‹ä¼šè¯...")
                home_response = client.get("https://www.zhihu.com/", timeout=15)
                if home_response.status_code == 200:
                    print("httpx: çŸ¥ä¹é¦–é¡µè®¿é—®æˆåŠŸï¼Œè·å–cookies...")
                    time.sleep(random.uniform(2, 4))
                    
                    # å°è¯•è®¿é—®ä¸“æ é¦–é¡µ
                    print("httpx: è®¿é—®ä¸“æ é¦–é¡µ...")
                    column_response = client.get("https://zhuanlan.zhihu.com/", timeout=15)
                    if column_response.status_code == 200:
                        print("httpx: ä¸“æ é¦–é¡µè®¿é—®æˆåŠŸ...")
                        time.sleep(random.uniform(2, 4))
                    else:
                        print("httpx: ä¸“æ é¦–é¡µè®¿é—®å¤±è´¥ï¼Œä½†ç»§ç»­å°è¯•...")
                else:
                    print(f"httpx: çŸ¥ä¹é¦–é¡µè®¿é—®å¤±è´¥: {home_response.status_code}")
            except Exception as e:
                print(f"httpx: è®¿é—®çŸ¥ä¹é¦–é¡µå¤±è´¥: {e}")
            
            # è®¿é—®ç›®æ ‡é¡µé¢
            print("httpx: è®¿é—®ç›®æ ‡é¡µé¢...")
            response = client.get(url, timeout=30)
            
            if response.status_code >= 400:
                return CrawlerResult(
                    success=False,
                    title=None,
                    text_content="",
                    error=f"HTTP {response.status_code}"
                )
            
            return CrawlerResult(
                success=True,
                title=extract_title_from_html(response.text),
                text_content=response.text
            )
    except ImportError:
        return CrawlerResult(
            success=False,
            title=None,
            text_content="",
            error="httpx not installed"
        )




def _get_zhihu_headers() -> dict:
    """è·å–çŸ¥ä¹ä¸“ç”¨è¯·æ±‚å¤´"""
    return {
        "User-Agent": random.choice(get_user_agents()),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Referer": "https://www.zhihu.com/",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "cross-site",
        "Sec-Fetch-User": "?1",
        "Cache-Control": "max-age=0",
        "sec-ch-ua": '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
    }

def _clean_zhihu_zhida_links(content_elem):
    """æ¸…ç†çŸ¥ä¹ç›´ç­”é“¾æ¥ï¼Œä¿ç•™æ–‡æœ¬å†…å®¹ï¼Œç§»é™¤é“¾æ¥"""
    import re
    from bs4 import NavigableString
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«çŸ¥ä¹ç›´ç­”é“¾æ¥çš„<a>æ ‡ç­¾
    zhida_links = content_elem.find_all('a', href=re.compile(r'https://zhida\.zhihu\.com/search\?'))
    
    for link in zhida_links:
        # è·å–é“¾æ¥æ–‡æœ¬
        link_text = link.get_text(strip=True)
        
        # å¦‚æœé“¾æ¥æ–‡æœ¬ä¸ä¸ºç©ºï¼Œç”¨çº¯æ–‡æœ¬æ›¿æ¢é“¾æ¥
        if link_text:
            # åˆ›å»ºçº¯æ–‡æœ¬èŠ‚ç‚¹
            text_node = NavigableString(link_text)
            # ç”¨çº¯æ–‡æœ¬æ›¿æ¢é“¾æ¥
            link.replace_with(text_node)
        else:
            # å¦‚æœé“¾æ¥æ–‡æœ¬ä¸ºç©ºï¼Œç›´æ¥ç§»é™¤
            link.decompose()
    
    # é¢å¤–å¤„ç†ï¼šæ¸…ç†å¯èƒ½å­˜åœ¨çš„å…¶ä»–çŸ¥ä¹å†…éƒ¨é“¾æ¥
    internal_links = content_elem.find_all('a', href=re.compile(r'https://www\.zhihu\.com/(question|answer|p)/'))
    
    for link in internal_links:
        link_text = link.get_text(strip=True)
        if link_text:
            text_node = NavigableString(link_text)
            link.replace_with(text_node)
        else:
            link.decompose()


def _clean_zhihu_external_links(content_elem):
    """æ¸…ç†çŸ¥ä¹å¤–éƒ¨é“¾æ¥é‡å®šå‘ï¼Œæ¢å¤åŸå§‹é“¾æ¥"""
    import re
    from urllib.parse import unquote, urlparse, parse_qs
    from bs4 import NavigableString
    
    # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«çŸ¥ä¹é‡å®šå‘é“¾æ¥çš„<a>æ ‡ç­¾
    redirect_links = content_elem.find_all('a', href=re.compile(r'https://link\.zhihu\.com/\?target='))
    
    for link in redirect_links:
        href = link.get('href', '')
        
        try:
            # è§£æURLå‚æ•°
            parsed_url = urlparse(href)
            query_params = parse_qs(parsed_url.query)
            
            # è·å–targetå‚æ•°
            if 'target' in query_params and query_params['target']:
                target_url = query_params['target'][0]
                # URLè§£ç 
                decoded_url = unquote(target_url)
                
                # æ›´æ–°é“¾æ¥çš„hrefå±æ€§
                link['href'] = decoded_url
                print(f"Playwright: æ¢å¤å¤–éƒ¨é“¾æ¥: {href} -> {decoded_url}")
            else:
                # å¦‚æœtargetå‚æ•°ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œå°†é“¾æ¥è½¬æ¢ä¸ºçº¯æ–‡æœ¬
                link_text = link.get_text(strip=True)
                if link_text:
                    text_node = NavigableString(link_text)
                    link.replace_with(text_node)
                    print(f"Playwright: è½¬æ¢æ— æ•ˆé‡å®šå‘é“¾æ¥ä¸ºçº¯æ–‡æœ¬: {href}")
                else:
                    link.decompose()
                    print(f"Playwright: ç§»é™¤ç©ºçš„é‡å®šå‘é“¾æ¥: {href}")
                
        except Exception as e:
            print(f"Playwright: å¤„ç†é‡å®šå‘é“¾æ¥å¼‚å¸¸: {e}")
            # å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè½¬æ¢ä¸ºçº¯æ–‡æœ¬
            link_text = link.get_text(strip=True)
            if link_text:
                text_node = NavigableString(link_text)
                link.replace_with(text_node)
            else:
                link.decompose()
            continue


def _fix_zhihu_image_formats(content_elem):
    """ä¿®å¤çŸ¥ä¹å›¾ç‰‡æ ¼å¼é—®é¢˜ï¼Œæ£€æµ‹å¹¶ä¿®æ­£WebPæ ¼å¼"""
    import re
    import httpx
    from urllib.parse import urlparse
    
    # æŸ¥æ‰¾æ‰€æœ‰çŸ¥ä¹å›¾ç‰‡
    zhihu_images = content_elem.find_all('img', src=re.compile(r'https://pic\d+\.zhimg\.com/'))
    
    for img in zhihu_images:
        src = img.get('src', '')
        if not src:
            continue
            
        try:
            # æ£€æŸ¥å›¾ç‰‡æ ¼å¼
            print(f"Playwright: æ£€æµ‹å›¾ç‰‡æ ¼å¼: {src}")
            
            # ä½¿ç”¨httpxå‘é€HEADè¯·æ±‚è·å–Content-Type
            with httpx.Client(timeout=10.0, follow_redirects=True) as client:
                response = client.head(src)
                content_type = response.headers.get('content-type', '').lower()
            
            # è§£æURL
            parsed_url = urlparse(src)
            path = parsed_url.path
            filename = path.split('/')[-1] if path else ''
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯WebPæ ¼å¼ä½†æ‰©å±•åä¸æ˜¯.webp
            if 'image/webp' in content_type and not filename.endswith('.webp'):
                # ä¿®æ­£æ‰©å±•å
                if '.' in filename:
                    name, ext = filename.rsplit('.', 1)
                    new_filename = f"{name}.webp"
                else:
                    new_filename = f"{filename}.webp"
                
                # æ„å»ºæ–°çš„URL
                new_path = path.replace(filename, new_filename)
                new_src = f"{parsed_url.scheme}://{parsed_url.netloc}{new_path}"
                if parsed_url.query:
                    new_src += f"?{parsed_url.query}"
                
                # æ›´æ–°å›¾ç‰‡src
                img['src'] = new_src
                print(f"Playwright: ä¿®æ­£å›¾ç‰‡æ ¼å¼: {src} -> {new_src}")
                
            elif 'image/jpeg' in content_type or 'image/jpg' in content_type:
                print(f"Playwright: å›¾ç‰‡æ ¼å¼æ­£ç¡® (JPEG): {src}")
            elif 'image/png' in content_type:
                print(f"Playwright: å›¾ç‰‡æ ¼å¼æ­£ç¡® (PNG): {src}")
            else:
                print(f"Playwright: æœªçŸ¥å›¾ç‰‡æ ¼å¼: {content_type} - {src}")
                
        except Exception as e:
            print(f"Playwright: æ£€æµ‹å›¾ç‰‡æ ¼å¼å¼‚å¸¸: {e}")
            continue


def _process_zhihu_content(html: str, title: str | None = None, url: str | None = None) -> FetchResult:
    """å¤„ç†çŸ¥ä¹å†…å®¹ï¼Œæå–æ ‡é¢˜ã€ä½œè€…ã€å‘å¸ƒæ—¥æœŸå’Œæ­£æ–‡"""
    try:
        soup = BeautifulSoup(html, 'lxml')
    except Exception as e:
        print(f"BeautifulSoupè§£æå¤±è´¥: {e}")
        return FetchResult(title=None, html_markdown="")

    # æŸ¥æ‰¾æ ‡é¢˜ - çŸ¥ä¹ä¸“æ çš„æ ‡é¢˜é€‰æ‹©å™¨
    if not title:
        title_elem = (
            soup.find('h1', class_='Post-Title') or
            soup.find('h1', class_='ArticleItem-title') or
            soup.find('h1', class_='Post-Title') or
            soup.find(attrs={'property': 'og:title'}) or
            soup.find(attrs={'property': 'twitter:title'}) or
            soup.find('h1') or
            soup.find('title')
        )
        if title_elem:
            title = getattr(title_elem, 'get', lambda *_: None)('content') or title_elem.get_text(strip=True)

    # æŸ¥æ‰¾ä½œè€…ä¿¡æ¯
    author = None
    # é¦–å…ˆå°è¯•ä»metaæ ‡ç­¾è·å–
    author_meta = soup.find('meta', {'name': 'author'}) or soup.find('meta', {'property': 'article:author'})
    if author_meta:
        author = author_meta.get('content', '').strip()
    
    # å¦‚æœmetaæ ‡ç­¾æ²¡æœ‰ä½œè€…ä¿¡æ¯ï¼Œå°è¯•ä»æ–‡ç« å†…å®¹ä¸­æå–
    if not author:
        # æŸ¥æ‰¾åŒ…å«ä½œè€…ä¿¡æ¯çš„æ–‡æœ¬æ¨¡å¼
        content_elem = soup.find('div', class_='Post-RichTextContainer') or soup.find('div', class_='Post-RichText')
        if content_elem:
            # æŸ¥æ‰¾åŒ…å«"ä½œè€…ï¼š"ã€"æ–‡/"ç­‰å…³é”®è¯çš„æ–‡æœ¬
            author_patterns = [
                r'ä½œè€…[ï¼š:]\s*([^**\n]+)',
                r'æ–‡/.*?ï¼š([^**\n]+)',
                r'ç¼–è¾‘[ï¼š:]\s*([^**\n]+)',
                r'æ¥æº[ï¼š:]\s*([^**\n]+)'
            ]
            
            import re
            content_text = content_elem.get_text()
            for pattern in author_patterns:
                match = re.search(pattern, content_text)
                if match:
                    author = match.group(1).strip()
                    # æ¸…ç†ä½œè€…åç§°ï¼Œç§»é™¤å¤šä½™ä¿¡æ¯
                    author = re.sub(r'ï¼ˆ.*?ï¼‰', '', author)  # ç§»é™¤æ‹¬å·å†…å®¹
                    author = re.sub(r'çŸ¥ä¹.*', '', author)  # ç§»é™¤çŸ¥ä¹ä¿¡æ¯
                    author = author.strip()
                    if author:
                        break

    # æŸ¥æ‰¾å‘å¸ƒæ—¥æœŸ
    publish_date = None
    date_selectors = [
        'time[datetime]',
        'span[data-tooltip]',
        'div.Post-Header .ContentItem-time',
        'meta[property="article:published_time"]',
        'meta[name="publish_time"]'
    ]
    
    for selector in date_selectors:
        elem = soup.select_one(selector)
        if elem:
            if elem.name == 'meta':
                publish_date = elem.get('content', '').strip()
            elif elem.get('datetime'):
                publish_date = elem.get('datetime', '').strip()
            else:
                publish_date = elem.get_text(strip=True)
            if publish_date:
                break

    # æŸ¥æ‰¾å†…å®¹åŒºåŸŸ - çŸ¥ä¹ä¸“æ çš„å†…å®¹é€‰æ‹©å™¨
    content_elem = None
    content_selectors = [
        'div.Post-RichTextContainer',
        'div[data-zop-feedtype]',
        'div.Post-RichText',
        'div.ArticleItem-content',
        'div.entry-content',
        'article',
        'div.content'
    ]
    
    for selector in content_selectors:
        content_elem = soup.select_one(selector)
        if content_elem:
            break
    
    if content_elem:
        # å¤„ç†çŸ¥ä¹ç‰¹æœ‰çš„å›¾ç‰‡æ‡’åŠ è½½
        for img in content_elem.find_all('img', {'data-src': True}):
            img['src'] = img['data-src']
            del img['data-src']
        
        # å¤„ç†çŸ¥ä¹çš„å›¾ç‰‡å ä½ç¬¦
        for img in content_elem.find_all('img', {'data-original': True}):
            img['src'] = img['data-original']
            del img['data-original']
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in content_elem.find_all(['script', 'style']):
            script.decompose()
        
        # ç§»é™¤çŸ¥ä¹ç‰¹æœ‰çš„æ— ç”¨å…ƒç´ 
        for elem in content_elem.find_all(['div'], class_=['Post-RichTextContainer']):
            # ä¿ç•™ä¸»è¦å†…å®¹ï¼Œç§»é™¤å¹¿å‘Šç­‰
            pass
        
        # ç§»é™¤çŸ¥ä¹çš„æ¨èå†…å®¹
        for elem in content_elem.find_all(['div'], class_=['Recommendation-Main', 'Card', 'Card--padding']):
            elem.decompose()
        
        # æ¸…ç†çŸ¥ä¹ç›´ç­”é“¾æ¥ - ä¿ç•™æ–‡æœ¬ï¼Œç§»é™¤é“¾æ¥
        _clean_zhihu_zhida_links(content_elem)
        
        # æ¸…ç†çŸ¥ä¹å¤–éƒ¨é“¾æ¥é‡å®šå‘ - æ¢å¤åŸå§‹é“¾æ¥
        _clean_zhihu_external_links(content_elem)
        
        # ä¿®å¤çŸ¥ä¹å›¾ç‰‡æ ¼å¼é—®é¢˜ - æ£€æµ‹å¹¶ä¿®æ­£WebPæ ¼å¼
        _fix_zhihu_image_formats(content_elem)
        
        md = html_fragment_to_markdown(content_elem)
    else:
        md = ""

    # æ„å»ºå®Œæ•´çš„markdownå†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜ã€URLã€ä½œè€…ã€å‘å¸ƒæ—¥æœŸ
    header_parts = []
    
    # æ·»åŠ æ ‡é¢˜
    if title:
        header_parts.append(f"# {title}")
    
    # æ·»åŠ æ–‡ç« é“¾æ¥URL
    if url:
        header_parts.append(f"**æ–‡ç« é“¾æ¥ï¼š** {url}")
    
    # æ·»åŠ ä½œè€…å’Œå‘å¸ƒæ—¥æœŸä¿¡æ¯
    if author or publish_date:
        meta_info = []
        if author:
            meta_info.append(f"**ä½œè€…ï¼š** {author}")
        if publish_date:
            meta_info.append(f"**å‘å¸ƒæ—¶é—´ï¼š** {publish_date}")
        
        if meta_info:
            header_parts.append("\n".join(meta_info))
    
    # å¦‚æœæœ‰æ ‡é¢˜æˆ–å…ƒä¿¡æ¯ï¼Œæ·»åŠ åˆ°markdownå¼€å¤´
    if header_parts:
        header = "\n\n".join(header_parts) + "\n\n"
        md = header + md if md else header
    
    # å¦‚æœæ‰¾åˆ°äº†å†…å®¹ä½†æ²¡æœ‰æ ‡é¢˜ï¼Œå°è¯•ä»é¡µé¢å…¶ä»–åœ°æ–¹è·å–
    elif not title and md:
        # å°è¯•ä»metaæ ‡ç­¾è·å–
        meta_title = soup.find('meta', {'property': 'og:title'})
        if meta_title:
            title = meta_title.get('content', '').strip()
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰ï¼Œå°è¯•ä»é¡µé¢æ ‡é¢˜è·å–
        if not title:
            page_title = soup.find('title')
            if page_title:
                title = page_title.get_text(strip=True)

        if title:
            md = (f"# {title}\n\n" + md) if md else f"# {title}\n\n"
    
    try:
        return FetchResult(title=title, html_markdown=md)
    except Exception as e:
        print(f"åˆ›å»ºFetchResultå¤±è´¥: {e}")
        return FetchResult(title=None, html_markdown="")
